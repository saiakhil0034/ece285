{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Generative versus Discriminative models in a simple binary classification problem\n",
    "\n",
    "The purpose of this notebook is to compare two different approaches for binary classification. We will consider a toy example: classifying male and female individuals based on their heights.\n",
    "\n",
    "One model will be based on a generative model, the other one on a discriminative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will simulate a training set.\n",
    "\n",
    "Let $N$ be the number of individuals in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to simulate a population of individuals with\n",
    "- feature: $x$ the height,\n",
    "- label: $d$ the gender (0:female, 1:male),\n",
    "\n",
    "We want to predict the label $d$ from the feature $x$.\n",
    "\n",
    "Following https://www.johndcook.com/blog/2008/11/25/distribution-of-adult-heights/\n",
    "- $x|d=0$ is Gaussian distributed with mean 64 and standard deviation 3\n",
    "- $x|d=1$ is Gaussian distributed with mean 70 and standard deviation 3\n",
    "- $p(d=0) = .5$\n",
    "- $p(d=1) = .5$\n",
    "\n",
    "The following function generates N samples according to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(N):\n",
    "    d = np.random.choice([0, 1], size=N, p=(.5, .5))\n",
    "    x = np.random.randn(N);\n",
    "    x[d == 0] = 64 + 3 * x[d == 0];\n",
    "    x[d == 1] = 70 + 3 * x[d == 1];\n",
    "    return x, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate a collection of $N$ such pairs $(x, d)$ that consititute our training set $\\mathcal{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, d = simulate_data(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the distribution of our population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(x, bins=100)\n",
    "plt.xlabel('height')\n",
    "plt.ylabel('number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a generative model we want to learn the distribution of heights within the female and male classes to make a decision based on Bayes rule.\n",
    "\n",
    "We will use the knowledge that the distribution within each class is Gaussian. Estimating these distributions then boils down at estimating their means $\\mu = (\\mu_1, \\mu_2)$ and standard deviations $\\sigma = (\\sigma_1, \\sigma_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu   = np.array([x[d == k].mean() for k in [0, 1]])\n",
    "sig  = np.array([x[d == k].std() for k in [0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these statistics to define the likelihoods:\n",
    "\n",
    "$p(x|d=k)=\\mathcal{N}(x;\\mu_k,\\sigma_k)$\n",
    "\n",
    "where $\\mathcal{N}$ is the Gaussian distribution whose implementation is available in scipy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lik = lambda x, k : sps.norm(mu[k], sig[k]).pdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to estimate the priors $p(d=0)$ and $p(d=0)$, which are simply the frequencies $f = (f_0, f_1)$ of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = np.array([(d == k).mean() for k in [0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these statistics to define the joint distributions:\n",
    "\n",
    "$p(x, d=k) = p(d=k) p(x | d=k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = lambda x, k : freq[k] * lik(x, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can deduce the marginal density of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal = lambda x : sum(joint(x, l) for l in range(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that all our models fit the distribution of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 100\n",
    "xgrid = np.linspace(x.min(), x.max(), nbins)\n",
    "fig, axes = plt.subplots(ncols=3, figsize=[3*6.4, 4.8])\n",
    "axes[0].hist(x[d == 0], bins=nbins, density=True, label='data')\n",
    "axes[1].hist(x[d == 1], bins=nbins, density=True, label='data')\n",
    "axes[2].hist(x, bins=nbins, density=True, label='data')\n",
    "axes[0].plot(xgrid, lik(xgrid, 0), label='p(x|female)')\n",
    "axes[1].plot(xgrid, lik(xgrid, 1), label='p(x|male)')\n",
    "axes[2].plot(xgrid, marginal(xgrid), label='p(x)')\n",
    "axes[2].plot(xgrid, joint(xgrid, 0), label='p(x, female)')\n",
    "axes[2].plot(xgrid, joint(xgrid, 1), label='p(x, male)')\n",
    "for k in range(3):\n",
    "    axes[k].set_xlabel('height $x$')\n",
    "    axes[k].set_ylabel('normalized proportion')\n",
    "    axes[k].legend()\n",
    "axes[0].set_title('Class of female')\n",
    "axes[1].set_title('Class of male')\n",
    "axes[2].set_title('Overall population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can deduce the posterior distributions using Bayes rule\n",
    "\n",
    "$p(d=k|x) = \\frac{p(x, d=k)}{p(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = lambda x, k : joint(x, k) / marginal(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the posterior probabilities as a function of the height $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(xgrid, post(xgrid, 0), label='p(d=0 | x)')\n",
    "plt.plot(xgrid, post(xgrid, 1), label='p(d=1 | x)')\n",
    "plt.xlabel('height  $x$')\n",
    "plt.ylabel('probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our maximum a posterior classifier for $d$\n",
    "\n",
    "$$\n",
    "f_{\\mathrm{gen}}(x; \\theta_{\\mathrm{gen}}) = \\arg \\max_k p_{\\theta_{\\mathrm{gen}}}(d=k | x) \n",
    "$$\n",
    "\n",
    "where we made explicit the dependency of the posterior with the parameters of our model being:\n",
    "\n",
    "$$\n",
    "\\theta_{\\mathrm{gen}} = (f_1, f_2, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2)\n",
    "\\quad\\text{ subject to }\\quad\n",
    "f_1 + f_2 = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_gen = lambda x: post(x, 1) > post(x, 0)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xgrid, f_gen(xgrid), label='$f_{\\mathrm{gen}}(x)$')\n",
    "plt.xlabel('height $x$')\n",
    "plt.ylabel('prediction $y_{\\mathrm{gen}}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this classifier makes a prediction for $d$ based on a simple thresholding\n",
    "\n",
    "$$\n",
    "f_{\\rm gen}(x; \\theta_{\\mathrm{gen}}) = \\left\\{\\begin{array}{ll}\n",
    "1 & \\text{if } x > t\\\\ \n",
    "0 & \\text{otherwise}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "where $t \\approx \\tfrac12 (70 + 64) = 67$. In fact $t$ converges in probablity to this value with respect to $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that though the loss function is not explicit here, the formulas for the estimation of the models result from optimizing the likelihood of the model parameters on $(x, d)$:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta_{\\mathrm{gen}}} \\prod_{(x, d) \\in \\mathcal{T}} p_{\\theta_{\\mathrm{gen}}}(x, d)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the discriminative approach we do not focus on the distribution of the features. Instead we directly impose a model on the posterior probability. For instance, based on our knowledge that males are in average larger than women, we can choose the following model:\n",
    "\n",
    "$$\n",
    "p_t(d=1 | x)  = \\left\\{\\begin{array}{ll}\n",
    "1 & \\text{if } x > t\\\\ \n",
    "0 & \\text{otherwise}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p_t(d=0 | x) = 1 - p_t(d=1 | x)\n",
    "$$\n",
    "\n",
    "The maximum a posteriori classifier is then\n",
    "\n",
    "$$\n",
    "f_{\\mathrm{dis}}(x; \\theta_{\\mathrm{dis}} = t) = p_t(d=1 | x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\theta_{\\mathrm{dis}}$ of that model is just the threshold.\n",
    "\n",
    "We will consider the number of errors on the training set as our loss function\n",
    "\n",
    "$$\n",
    "\\min_{\\theta_{\\mathrm{dis}}}\n",
    "\\#\\{(x, d) \\in \\mathcal{T} \\text{ such that } f_{\\mathrm{dis}}(x; \\theta_{\\mathrm{dis}}) \\ne d\\}\n",
    "$$\n",
    "\n",
    "Clearly this optimization problem can be recasted as\n",
    "\n",
    "$$\n",
    "\\min_{t}\n",
    "\\underbrace{\\#\\{(x, d) \\in \\mathcal{T} \\text{ such that } x \\le t \\text{ and } d = 1\\}}_{\\text{number of missclassified males}}\n",
    "+\n",
    "\\underbrace{\\#\\{(x, d) \\in \\mathcal{T} \\text{ such that } x > t \\text{ and } d = 0\\}}_{\\text{number of missclassified females}}\n",
    "$$\n",
    "Or equivalently\n",
    "$$\n",
    "\\min_{t}\n",
    "\\sum_{(x, d) \\in \\mathcal{T} ; x \\le t}\n",
    "d\n",
    "+\n",
    "\\sum_{(x, d) \\in \\mathcal{T} ; x > t}\n",
    "1 - d\n",
    "$$\n",
    "\n",
    "Now if we denote by $x_k$ the $k$-th smaller individual in $\\mathcal{T}$ and $d_k$ its corresponding label, we can show that any solution $t^*$ of the above optimization problem are given by\n",
    "$$\n",
    "t^* \\in [x_{k^*}, x_{k^*+1}) \\quad\\text{where}\\quad\n",
    "k^* = \\arg \\min_{1 \\le k \\le N-1}\n",
    "\\sum_{i=1}^k d_k\n",
    "+\n",
    "\\sum_{i=k+1}^N 1 - d_k\n",
    "$$\n",
    "\n",
    "which leads us to consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(x);\n",
    "sorted_x = x[idx];\n",
    "sorted_d = d[idx];\n",
    "nb_missclassified_males = np.cumsum(sorted_d)\n",
    "nb_missclassified_females = \\\n",
    "    np.sum(1-sorted_d) - np.cumsum(1 - sorted_d)\n",
    "nb_errors = nb_missclassified_males + nb_missclassified_females \n",
    "k = np.argmin(nb_errors)\n",
    "t_star = (sorted_x[k] + sorted_x[k+1]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display the loss as a function of $t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sorted_x, nb_missclassified_males, \\\n",
    "         label='#missclassified males')\n",
    "plt.plot(sorted_x, nb_missclassified_females, \\\n",
    "         label='#missclassified females')\n",
    "plt.plot(sorted_x, nb_errors, label='#errors')\n",
    "plt.axvline(t_star, color='k', label='optimal threshold $t^*$')\n",
    "plt.xlabel('threshold $t$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define and visualize our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dis = lambda x: x > t_star\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xgrid, f_dis(xgrid), label='$f_{\\mathrm{dis}}(x)$')\n",
    "plt.xlabel('height $x$')\n",
    "plt.ylabel('prediction $y_{\\mathrm{gen}}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for such problems, a popular alternative is to model $p(d=1|x)$ with the logisitic function, to consider the cross-entropy loss, and to optimize the parameter with gradient descent, as in Assignment 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "Let us compare the two models on a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntest = 1000000\n",
    "xtest, dtest = simulate_data(Ntest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = f_gen(xtest)\n",
    "gen_perf = (ytest == dtest).mean()\n",
    "print('Success rate of generative model: %.6f' % gen_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = f_dis(xtest)\n",
    "dis_perf = (ytest == dtest).mean()\n",
    "print('Success rate of discriminative model: %.6f' % dis_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best model is: %s' % \\\n",
    "      ('generative' if gen_perf > dis_perf else 'discriminative'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario: data in low dimensional space ($x \\in \\mathbb{R}$), and perfect knowledge on the model of data distribution, the two approaches reach almost the same performance (though the generative seems to be winning more frequently).\n",
    "\n",
    "This observation will rapidly become invalid as we consider a feature space of larger dimension $x \\in \\mathbb{R}^d, d \\gg 1$ or if the assumed generative model does not fit well the data.\n",
    "\n",
    "This is because it is easier to model/estimate the shape of a separator than to model/estimate the distribution of each classes, all the more in highdimensional spaces. In this example, estimating the separator requires to estimate one parameter (the threshold $t$), but estimating the class distributions requires to estimate 6 parameters (the means $\\mu_k$, standard deviations $\\sigma_k$ and frequencies $f_k$). The larger the number of parameters to estimate, the more challenging is the estimation (learning subject to overfitting). Since overfitting is more common in high dimensional feature spaces (curse of dimensionality), generative methods are usually defeated by discriminative ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-3.6]",
   "language": "python",
   "name": "conda-env-pytorch-3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
